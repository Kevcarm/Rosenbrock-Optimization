	The problem that was decided on for the project was an optimization problem with the Rosenbrock function. Rosenbrock is mathematical optimization that is a non-convex function, introduced by Howard H. Rosenbrock in 1960. It is used as a performance test problem for optimization algorithms. The function can be efficiently optimized by adapting appropriate coordinate systems without using any gradient information and without building local approximation models. It's a popular choice to pick for testing algorithms because it is easy to follow the valley of the function down, but it is hard to find its local minima.

	Rosenbrock was chosen as it seemed like it was a function with a lot of documentation behind it and only had one global minima but was hard to find. Using Rosenbrock with an optimization problem would allow us to test evolutionary computations with it to see which one could find the global minima in the shortest amount of time. For this, four evolutionary computations were planned to be used for this problem. These were simulated annealing, (1+1) EA, randomized hill climbing, and tabu search. These were chosen as they seemed best fit to find the global minima in the Rosenbrock function. In the end, only two of the four evolutionary computations were used to test which could be the fastest to find the global minima.

	The evolutionary computations that were used for the Rosenbrock function were simulated annealing and (1+1) EA. Simulated Annealing was used to help test Rosenbrock for the global minima; the same thing went with the (1+1) EA. The reason these were chosen was because they were the only ones that worked for the real value function. The other two that were originally planned to be used but were not were randomized hill climbing and tabu search. Randomized hill climbing had to be cut because it mainly worked well for discrete values. This meant that finding the minima in a Rosenbrock function would not work with it. For tabu search, it does not work well with valley functions. Rosenbrock is a valley function, and after some research for tabu search it was decided it would not work with finding the minima in the optimization problem.

	Simulated annealing and (1+1) EA were being used to compare the best minimum and CPU time. Three different mutation operations were used to help solve the Rosenbrock optimization problem. These mutations were gaussian, cauchy, and uniform. The mutation for gaussian adds random values drawn from normal gaussians to the parameters. The mutation for cauchy was used to help improve the performance for the global search. The mutation for uniform helps prevent the algorithm from getting stuck in the local optima during the optimization process. Different cooling methods were used, these being log, exponential and linear. These cooling methods are components of simulated annealing, which is why they are not used for the (1+1) EA. The cooling methods help by reducing the infrequency in the search itself.

	The results are split up into two sections, optimizations results, the cost values, and optimization time, in seconds. Each section is sorted by variants for gaussian, cauchy, and uniform for both SA and EA with SA including all three cooling methods with exponential, linear, and log. Figure 1 shows the optimization results of cost values showing all the results for SA and EA. For SA, uniform with the exponential cooling method showed to work the fastest and in EA, gaussian was shown to be faster. Overall, the EA gaussian was shown to perform the best out of the two. Figure 2 shows the optimization time in seconds, showing all the results for SA and EA. For SA, uniform with the exponential cooling method was shown to perform the best and in EA, uniform was shown to perform the best. Overall, EA uniform was shown to perform the best out of the two results. From these results, the best method for both cost values and time were in EA, with cost value performing best with gaussian and time performing best with uniform. This shows that in the Rosenbrock optimization problem, EA as an evolutionary algorithm is shown to perform the best compared to SA and the other operations.

	From the information gathered from this project, (1+1) EA was shown to perform the best for both cost values and time with cost values performing best with gaussian and time performing best with uniform. This has shown that the best evolutionary computation to use for the Rosenbrock function is (1+1) EA. Although, the operation must be different for the two due to gaussian performing the best with time and uniform performing best with time.

References:
Cicirello, V. (n.d.). N-salsa - a java library of customizable, hybridizable, iterative, parallel, stochastic, and self-adaptive local search algorithms. Chips-N-Salsa. https://chips-n-salsa.cicirello.org/ 
